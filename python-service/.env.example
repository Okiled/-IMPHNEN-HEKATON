# ===========================================
# Megaw AI ML Service - Environment Configuration
# ===========================================
# Copy this file to .env and fill in your values
# NEVER commit your .env file to version control!

# ===========================================
# Server Configuration
# ===========================================
PORT=8000
HOST=0.0.0.0
ENV=development

# ===========================================
# CORS Configuration
# ===========================================
# Comma-separated list of allowed origins
# Leave empty to use defaults based on ENV

ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5000

# Production URLs (used when ENV=production)
FRONTEND_URL=https://megaw-ai.vercel.app
BACKEND_URL=https://api.megaw-ai.com

# ===========================================
# Model Configuration
# ===========================================
MODEL_PATH=./models/artifacts
MODEL_VERSION=1.1.0
MAX_PREDICTION_BATCH=1000

# ===========================================
# Cache Configuration
# ===========================================
ENABLE_MODEL_CACHING=true
CACHE_TTL_SECONDS=3600
MAX_CACHED_MODELS=50

# ===========================================
# Logging
# ===========================================
LOG_LEVEL=INFO

# ===========================================
# HuggingFace Integration (optional - for future migration)
# ===========================================
# HF_API_TOKEN=your_huggingface_api_token_here
# HF_MODEL_ENDPOINT=https://api-inference.huggingface.co/models/your-model
